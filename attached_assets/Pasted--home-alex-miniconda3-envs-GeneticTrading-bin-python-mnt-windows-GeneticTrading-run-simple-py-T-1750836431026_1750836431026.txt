/home/alex/miniconda3/envs/GeneticTrading/bin/python /mnt/windows/GeneticTrading/run_simple.py 
ðŸ¤– Trading System Launcher
==================================================
âœ“ Created directory: ./models
âœ“ Created directory: ./logs
âœ“ Created directory: ./cached_data
âœ“ Created directory: ./runs

Available modes:
  test - Quick test with minimal data (default)
  dev  - Development mode with 10% data

Usage: python run_simple.py [test|dev]

Select mode (test/dev) [test]: dev
ðŸ”§ Starting Development Mode...
ðŸ“Š Using 10% of data for development
WARNING:root:Distributed training not available, running in single-process mode
2025-06-25 00:26:44 [INFO    ] [Rank-11274] STARTUP             : ================================================================================
2025-06-25 00:26:44 [INFO    ] [Rank-11274] STARTUP             : Training session started for rank 0
2025-06-25 00:26:44 [INFO    ] [Rank-11274] STARTUP             : Log file: logs/training_rank_0.log
2025-06-25 00:26:44 [INFO    ] [Rank-11274] STARTUP             : Process ID: 11274
2025-06-25 00:26:44 [INFO    ] [Rank-11274] STARTUP             : Working directory: /mnt/windows/GeneticTrading
2025-06-25 00:26:44 [INFO    ] [Rank-11274] STARTUP             : ================================================================================
2025-06-25 00:26:44 [INFO    ] [Rank-11274] root                : NCCL_TIMEOUT = 1800000 ms
2025-06-25 00:26:44 [INFO    ] [Rank-11274] root                : Using 10.0% of available data
2025-06-25 00:26:44 [INFO    ] [Rank-11274] root                : Models will be saved to: ./models/dev
2025-06-25 00:26:44 [INFO    ] [Rank-11274] root                : Rank 0/1 starting on cuda:0 (has_cudf=True)
2025-06-25 00:26:44 [INFO    ] [Rank-11274] root                : Parquet cache found; skipping preprocessing.
2025-06-25 00:26:44 [INFO    ] [Rank-11274] root                : Total data: 4311800 train, 1077950 test rows
2025-06-25 00:26:44 [INFO    ] [Rank-11274] root                : Rank 0: Sampled 100000 train rows from 4311800 total
2025-06-25 00:26:44 [INFO    ] [Rank-11274] root                : Rank 0: Sampled 20000 test rows from 1077950 total
2025-06-25 00:26:44 [INFO    ] [Rank-11274] numba.cuda.cudadrv.driver: init
2025-06-25 00:26:46 [INFO    ] [Rank-11274] adaptive_trainer    : Starting adaptive training
2025-06-25 00:26:46 [INFO    ] [Rank-11274] adaptive_trainer    : 
=== Adaptive Training Iteration 1/20 ===
2025-06-25 00:26:46 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:26:46 [ERROR   ] [Rank-11274] adaptive_trainer    : Error during policy evaluation: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)
2025-06-25 00:26:46 [INFO    ] [Rank-11274] adaptive_trainer    : Current performance: 0.0000 (best: 0.0000)
2025-06-25 00:26:46 [INFO    ] [Rank-11274] adaptive_trainer    : Stagnation: 0, Poor performance: 0
2025-06-25 00:26:46 [INFO    ] [Rank-11274] adaptive_trainer    : Method: GA, Entropy: 0.0000
2025-06-25 00:26:46 [INFO    ] [Rank-11274] adaptive_trainer    : Switching to PPO due to: ga_solution_refinement
2025-06-25 00:26:46 [INFO    ] [Rank-11274] adaptive_trainer    : Switching from GA to PPO
2025-06-25 00:26:46 [INFO    ] [Rank-11274] adaptive_trainer    : Starting PPO phase: 150 updates
2025-06-25 00:26:46 [INFO    ] [Rank-11274] policy_gradient_methods: Loaded model from models/dev/ppo_models/adaptive_ppo_model.pth
I0625 00:26:46.939000 11274 site-packages/torch/distributed/nn/jit/instantiator.py:24] Created a temporary directory at /tmp/tmpugyc0os_
I0625 00:26:46.939000 11274 site-packages/torch/distributed/nn/jit/instantiator.py:75] Writing /tmp/tmpugyc0os_/_remote_module_non_scriptable.py
Removed old TensorBoard run: ./runs/ga_experiment
2025-06-25 00:26:47 [INFO    ] [Rank-11274] policy_gradient_methods: Loaded model from models/dev/ppo_models/adaptive_ppo_model.pth
2025-06-25 00:26:47 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:26:47 [ERROR   ] [Rank-11274] adaptive_trainer    : Error during policy evaluation: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:26:47 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:26:47 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:26:47 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:26:48 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:26:48 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:26:48 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:26:48 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:26:49 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:26:49 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:26:49 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:26:49 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:26:50 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:26:50 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:26:50 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:26:50 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:26:51 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:26:51 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:26:51 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:26:51 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:26:52 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:26:52 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:26:52 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
PPO Update Epochs:   0%|          | 0/4 [00:00<?, ?it/s]
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s]
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0498, value_loss=38.6772, entropy=1.0911]
Epoch 1 Batches:   6%|â–‹         | 1/16 [00:00<00:01,  7.75it/s, policy_loss=-0.0498, value_loss=38.6772, entropy=1.0911]
Epoch 1 Batches:   6%|â–‹         | 1/16 [00:00<00:01,  7.75it/s, policy_loss=-0.0377, value_loss=41.9767, entropy=1.0923]
Epoch 1 Batches:   6%|â–‹         | 1/16 [00:00<00:01,  7.75it/s, policy_loss=-0.0320, value_loss=30.4996, entropy=1.0935]
Epoch 1 Batches:   6%|â–‹         | 1/16 [00:00<00:01,  7.75it/s, policy_loss=-0.0439, value_loss=30.1520, entropy=1.0917]
Epoch 1 Batches:   6%|â–‹         | 1/16 [00:00<00:01,  7.75it/s, policy_loss=0.0021, value_loss=30.7941, entropy=1.0928] 
Epoch 1 Batches:   6%|â–‹         | 1/16 [00:00<00:01,  7.75it/s, policy_loss=0.0420, value_loss=29.4832, entropy=1.0912]
Epoch 1 Batches:   6%|â–‹         | 1/16 [00:00<00:01,  7.75it/s, policy_loss=-0.2368, value_loss=36.9292, entropy=1.0912]
Epoch 1 Batches:   6%|â–‹         | 1/16 [00:00<00:01,  7.75it/s, policy_loss=0.1340, value_loss=24.9936, entropy=1.0940] 
Epoch 1 Batches:   6%|â–‹         | 1/16 [00:00<00:01,  7.75it/s, policy_loss=0.0838, value_loss=36.8578, entropy=1.0937]
Epoch 1 Batches:   6%|â–‹         | 1/16 [00:00<00:01,  7.75it/s, policy_loss=-0.2291, value_loss=28.6423, entropy=1.0952]
Epoch 1 Batches:   6%|â–‹         | 1/16 [00:00<00:01,  7.75it/s, policy_loss=0.2140, value_loss=28.0893, entropy=1.0938] 
Epoch 1 Batches:   6%|â–‹         | 1/16 [00:00<00:01,  7.75it/s, policy_loss=0.0305, value_loss=28.4783, entropy=1.0926]
Epoch 1 Batches:   6%|â–‹         | 1/16 [00:00<00:01,  7.75it/s, policy_loss=-0.0669, value_loss=32.3016, entropy=1.0922]
Epoch 1 Batches:   6%|â–‹         | 1/16 [00:00<00:01,  7.75it/s, policy_loss=0.1611, value_loss=26.9938, entropy=1.0941] 
Epoch 1 Batches:   6%|â–‹         | 1/16 [00:00<00:01,  7.75it/s, policy_loss=-0.0307, value_loss=34.1958, entropy=1.0943]
Epoch 1 Batches:   6%|â–‹         | 1/16 [00:00<00:01,  7.75it/s, policy_loss=0.0618, value_loss=30.5343, entropy=1.0888] 
PPO Update Epochs:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  5.10it/s, avg_policy_loss=0.0001, avg_value_loss=31.8499, kl_div=-0.0004]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0187, value_loss=36.2955, entropy=1.0912]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0978, value_loss=34.1196, entropy=1.0954] 
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1644, value_loss=34.9587, entropy=1.0932]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0923, value_loss=29.6001, entropy=1.0928]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1067, value_loss=26.2013, entropy=1.0917]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0733, value_loss=27.3198, entropy=1.0929]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1194, value_loss=35.7292, entropy=1.0914] 
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0622, value_loss=29.5256, entropy=1.0951]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1172, value_loss=26.3573, entropy=1.0921]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0231, value_loss=30.6159, entropy=1.0943]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0794, value_loss=34.1760, entropy=1.0921]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0781, value_loss=37.9661, entropy=1.0905]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0812, value_loss=33.6917, entropy=1.0924]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0346, value_loss=27.3828, entropy=1.0878]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0402, value_loss=32.2549, entropy=1.0917] 
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0594, value_loss=33.1094, entropy=1.0924]
PPO Update Epochs:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  5.10it/s, avg_policy_loss=-0.0004, avg_value_loss=31.8315, kl_div=-0.0002]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1619, value_loss=38.4635, entropy=1.0935]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0097, value_loss=30.1349, entropy=1.0923]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1149, value_loss=32.6933, entropy=1.0902]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1524, value_loss=33.4991, entropy=1.0899] 
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0382, value_loss=30.2187, entropy=1.0925]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0293, value_loss=36.2239, entropy=1.0925]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0584, value_loss=30.2310, entropy=1.0941]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1960, value_loss=33.1313, entropy=1.0924]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0074, value_loss=34.2305, entropy=1.0914] 
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0252, value_loss=25.6134, entropy=1.0933]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0319, value_loss=33.5210, entropy=1.0930] 
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0415, value_loss=32.6762, entropy=1.0923]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0364, value_loss=27.8381, entropy=1.0934]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0230, value_loss=25.9378, entropy=1.0939]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0275, value_loss=33.7806, entropy=1.0927]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1685, value_loss=31.1770, entropy=1.0933]
PPO Update Epochs:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  5.10it/s, avg_policy_loss=-0.0006, avg_value_loss=31.8356, kl_div=-0.0006]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1537, value_loss=27.6210, entropy=1.0934]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.2337, value_loss=38.4348, entropy=1.0926] 
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1289, value_loss=31.5127, entropy=1.0930]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1026, value_loss=28.0149, entropy=1.0930]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0395, value_loss=32.6555, entropy=1.0962]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0498, value_loss=32.9119, entropy=1.0941]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1916, value_loss=34.3083, entropy=1.0923]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0917, value_loss=31.0320, entropy=1.0940]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0284, value_loss=28.8289, entropy=1.0924]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1049, value_loss=22.2783, entropy=1.0942] 
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0981, value_loss=38.0262, entropy=1.0944]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0485, value_loss=29.3730, entropy=1.0924]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.2459, value_loss=31.5610, entropy=1.0924]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.2217, value_loss=33.2140, entropy=1.0920]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1226, value_loss=39.8537, entropy=1.0908]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0533, value_loss=29.4932, entropy=1.0943]
2025-06-25 00:26:53 [ERROR   ] [Rank-11274] policy_gradient_methods: Error in train_step: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:26:53 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:26:54 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:26:54 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:26:54 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:26:54 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:26:54 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:26:54 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:26:55 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:26:55 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:26:55 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:26:55 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:26:56 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:26:56 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:26:56 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:26:56 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:26:57 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:26:57 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:26:57 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:26:57 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:26:58 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
PPO Update Epochs:   0%|          | 0/4 [00:00<?, ?it/s]
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s]
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0463, value_loss=35.2492, entropy=1.0932]
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0152, value_loss=28.9920, entropy=1.0885]
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.2232, value_loss=34.9131, entropy=1.0925]
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0036, value_loss=25.2355, entropy=1.0943]
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1515, value_loss=36.9327, entropy=1.0923]
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0669, value_loss=37.9013, entropy=1.0912] 
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1375, value_loss=28.7766, entropy=1.0933]
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0489, value_loss=28.4209, entropy=1.0944]
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.2817, value_loss=31.6354, entropy=1.0947]
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1299, value_loss=29.0841, entropy=1.0929] 
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1065, value_loss=34.4841, entropy=1.0943]
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0110, value_loss=34.5035, entropy=1.0925] 
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0372, value_loss=30.5362, entropy=1.0935]
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0161, value_loss=24.3895, entropy=1.0951]
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1102, value_loss=31.3008, entropy=1.0913]
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0321, value_loss=25.1645, entropy=1.0931] 
PPO Update Epochs:   0%|          | 0/4 [00:00<?, ?it/s, avg_policy_loss=0.0003, avg_value_loss=31.0950, kl_div=0.0002]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0855, value_loss=29.0851, entropy=1.0935]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0490, value_loss=33.8645, entropy=1.0939]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0895, value_loss=26.9992, entropy=1.0931]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0143, value_loss=27.0941, entropy=1.0953] 
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1785, value_loss=32.8641, entropy=1.0929]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0661, value_loss=30.2725, entropy=1.0941]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0908, value_loss=28.6178, entropy=1.0959]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1038, value_loss=35.9316, entropy=1.0943] 
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0169, value_loss=30.2292, entropy=1.0930]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0643, value_loss=23.6751, entropy=1.0966]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0023, value_loss=27.2002, entropy=1.0942]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0098, value_loss=31.9864, entropy=1.0941]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0715, value_loss=30.1299, entropy=1.0929]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0336, value_loss=32.7487, entropy=1.0946]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.2675, value_loss=43.4668, entropy=1.0924]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.2179, value_loss=33.1911, entropy=1.0938]
PPO Update Epochs:   0%|          | 0/4 [00:00<?, ?it/s, avg_policy_loss=-0.0014, avg_value_loss=31.0848, kl_div=-0.0003]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1053, value_loss=27.1443, entropy=1.0973]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1816, value_loss=37.2812, entropy=1.0935]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1820, value_loss=32.9410, entropy=1.0919] 
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0326, value_loss=25.1643, entropy=1.0951]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.2325, value_loss=31.8696, entropy=1.0958] 
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0471, value_loss=34.2525, entropy=1.0945]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1064, value_loss=31.1196, entropy=1.0955] 
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0426, value_loss=27.9763, entropy=1.0938]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1088, value_loss=24.7439, entropy=1.0956]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1855, value_loss=30.0753, entropy=1.0960]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0916, value_loss=28.6305, entropy=1.0965]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0356, value_loss=29.8660, entropy=1.0965] 
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0858, value_loss=31.0164, entropy=1.0963]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0194, value_loss=31.1177, entropy=1.0957]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1166, value_loss=41.5188, entropy=1.0962] 
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0326, value_loss=32.3828, entropy=1.0964]
PPO Update Epochs:   0%|          | 0/4 [00:00<?, ?it/s, avg_policy_loss=-0.0031, avg_value_loss=31.0688, kl_div=-0.0003]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1115, value_loss=31.1404, entropy=1.0967]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0105, value_loss=39.5104, entropy=1.0956]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0178, value_loss=27.9900, entropy=1.0959] 
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1070, value_loss=30.4389, entropy=1.0960]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0265, value_loss=29.4327, entropy=1.0964]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0703, value_loss=32.5221, entropy=1.0965]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0098, value_loss=28.4015, entropy=1.0964] 
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0635, value_loss=29.5317, entropy=1.0960]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.2580, value_loss=30.4353, entropy=1.0966]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0100, value_loss=30.2850, entropy=1.0970]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0335, value_loss=31.2021, entropy=1.0972]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0679, value_loss=37.6263, entropy=1.0965] 
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0040, value_loss=32.2235, entropy=1.0960]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1615, value_loss=30.3315, entropy=1.0969]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1671, value_loss=27.0539, entropy=1.0969] 
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0482, value_loss=28.8975, entropy=1.0968]
2025-06-25 00:26:58 [ERROR   ] [Rank-11274] policy_gradient_methods: Error in train_step: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:26:58 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:26:59 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:26:59 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:26:59 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
2025-06-25 00:26:59 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:26:59 [ERROR   ] [Rank-11274] adaptive_trainer    : Error during policy evaluation: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)
2025-06-25 00:26:59 [INFO    ] [Rank-11274] adaptive_trainer    : PPO Update 25: Performance=0.0000, Entropy=0.0000
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:26:59 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:27:00 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:27:00 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:27:01 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:27:01 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:27:02 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:27:02 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:27:02 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:27:02 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:27:02 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:27:02 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:27:03 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:27:03 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:27:03 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:27:03 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:27:04 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:27:04 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:27:04 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:27:04 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:27:05 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:27:05 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:27:05 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:27:05 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:27:06 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:27:06 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:27:06 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:27:06 [WARNING ] [Rank-11274] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
