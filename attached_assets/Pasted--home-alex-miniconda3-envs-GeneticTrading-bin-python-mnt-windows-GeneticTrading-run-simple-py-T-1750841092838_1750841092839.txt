/home/alex/miniconda3/envs/GeneticTrading/bin/python /mnt/windows/GeneticTrading/run_simple.py 
ü§ñ Trading System Launcher
==================================================
‚úì Created directory: ./models
‚úì Created directory: ./logs
‚úì Created directory: ./cached_data
‚úì Created directory: ./runs

Available modes:
  test - Quick test with minimal data (default)
  dev  - Development mode with 10% data

Usage: python run_simple.py [test|dev]

Select mode (test/dev) [test]: dev
üîß Starting Development Mode...
üìä Using 10% of data for development
üìà Monitoring enabled - check ./logs/training_metrics.json for progress
‚ö†Ô∏è  Training will auto-stop if performance stagnates to save compute
üîß Starting Development Mode...
üìä Using 10% of data for development
WARNING:root:Distributed training not available, running in single-process mode
2025-06-25 01:44:39 [INFO    ] [Rank-22663] STARTUP             : ================================================================================
2025-06-25 01:44:39 [INFO    ] [Rank-22663] STARTUP             : Training session started for rank 0
2025-06-25 01:44:39 [INFO    ] [Rank-22663] STARTUP             : Log file: logs/training_rank_0.log
2025-06-25 01:44:39 [INFO    ] [Rank-22663] STARTUP             : Process ID: 22663
2025-06-25 01:44:39 [INFO    ] [Rank-22663] STARTUP             : Working directory: /mnt/windows/GeneticTrading
2025-06-25 01:44:39 [INFO    ] [Rank-22663] STARTUP             : ================================================================================
2025-06-25 01:44:39 [INFO    ] [Rank-22663] root                : NCCL_TIMEOUT = 1800000 ms
2025-06-25 01:44:39 [INFO    ] [Rank-22663] root                : Using 10.0% of available data
2025-06-25 01:44:39 [INFO    ] [Rank-22663] root                : Models will be saved to: ./models/dev
2025-06-25 01:44:39 [INFO    ] [Rank-22663] root                : Rank 0/1 starting on cuda:0 (has_cudf=True)
2025-06-25 01:44:39 [INFO    ] [Rank-22663] root                : Parquet cache found; skipping preprocessing.
2025-06-25 01:44:39 [INFO    ] [Rank-22663] root                : Total data: 400 train, 100 test rows
2025-06-25 01:44:39 [INFO    ] [Rank-22663] numba.cuda.cudadrv.driver: init
2025-06-25 01:44:39 [INFO    ] [Rank-22663] email_notifications : Email notifications configured for ali.aloraibi@outlook.com
2025-06-25 01:44:39 [INFO    ] [Rank-22663] email_notifications : Notifications will be sent every 6 hours
2025-06-25 01:44:39 [INFO    ] [Rank-22663] email_notifications : No training metrics available for progress update
2025-06-25 01:44:39 [INFO    ] [Rank-22663] email_notifications : Training monitoring started
2025-06-25 01:44:39 [INFO    ] [Rank-22663] root                : Email notifications enabled - reports every 6 hours
2025-06-25 01:44:39 [INFO    ] [Rank-22663] adaptive_trainer    : Removed old TensorBoard run: ./runs/ga_experiment
2025-06-25 01:44:39 [INFO    ] [Rank-22663] adaptive_trainer    : Starting adaptive training
2025-06-25 01:44:39 [INFO    ] [Rank-22663] adaptive_trainer    : 
=== Adaptive Training Iteration 1/20 ===
2025-06-25 01:44:39 [INFO    ] [Rank-22663] adaptive_trainer    : Evaluation results: 81 profits, total=-4289962.6611
2025-06-25 01:44:39 [INFO    ] [Rank-22663] adaptive_trainer    : Metrics: CAGR=0.0000, Sharpe=-5.0000, MDD=100.0000
2025-06-25 01:44:39 [INFO    ] [Rank-22663] adaptive_trainer    : Current performance: -0.4500 (best: -0.4500)
2025-06-25 01:44:39 [INFO    ] [Rank-22663] adaptive_trainer    : Stagnation: 0, Poor performance: 0
2025-06-25 01:44:39 [INFO    ] [Rank-22663] adaptive_trainer    : Method: GA, Entropy: 1.0986
2025-06-25 01:44:39 [INFO    ] [Rank-22663] adaptive_trainer    : Switching to PPO due to: ga_solution_refinement
2025-06-25 01:44:39 [INFO    ] [Rank-22663] adaptive_trainer    : Switching from GA to PPO
2025-06-25 01:44:39 [INFO    ] [Rank-22663] adaptive_trainer    : Starting PPO phase: 150 updates
2025-06-25 01:44:39 [INFO    ] [Rank-22663] policy_gradient_methods: No model file at models/dev/ppo_models/adaptive_ppo_model.pth, starting from scratch
I0625 01:44:39.773000 22663 site-packages/torch/distributed/nn/jit/instantiator.py:24] Created a temporary directory at /tmp/tmpx31fs387
I0625 01:44:39.773000 22663 site-packages/torch/distributed/nn/jit/instantiator.py:75] Writing /tmp/tmpx31fs387/_remote_module_non_scriptable.py
Removed old TensorBoard run: ./runs/ppo_rank_0
2025-06-25 01:44:40 [INFO    ] [Rank-22663] adaptive_trainer    : Evaluation results: 48 profits, total=-1591475.0329
2025-06-25 01:44:40 [INFO    ] [Rank-22663] adaptive_trainer    : Metrics: CAGR=0.0000, Sharpe=-5.0000, MDD=100.0000

Collecting Trajectories:   0%|                                             | 0/1024 [00:00<?, ?it/s]
Collecting Trajectories:   0%|    | 0/1024 [00:00<?, ?it/s, avg_reward=0.0, episodes=0, step=1/1024]
Collecting Trajectories:   0%| | 1/1024 [00:00<02:57,  5.78it/s, avg_reward=0.0, episodes=0, step=1/
Collecting Trajectories:   5%| | 56/1024 [00:00<00:03, 251.67it/s, avg_reward=0.0, episodes=0, step=
Collecting Trajectories:   5%| | 56/1024 [00:00<00:03, 251.67it/s, avg_reward=-133.0, episodes=0, st
Collecting Trajectories:   5%| | 56/1024 [00:00<00:03, 251.67it/s, avg_reward=26.9, episodes=1, step
Collecting Trajectories:  21%|‚ñè| 213/1024 [00:00<00:01, 767.12it/s, avg_reward=26.9, episodes=1, ste
Collecting Trajectories:  21%|‚ñè| 213/1024 [00:00<00:01, 767.12it/s, avg_reward=-1.9, episodes=0, ste
Collecting Trajectories:  36%|‚ñé| 368/1024 [00:00<00:00, 1045.79it/s, avg_reward=-1.9, episodes=0, st
Collecting Trajectories:  36%|‚ñé| 368/1024 [00:00<00:00, 1045.79it/s, avg_reward=26.2, episodes=0, st
Collecting Trajectories:  47%|‚ñç| 483/1024 [00:00<00:00, 946.16it/s, avg_reward=26.2, episodes=0, ste
Collecting Trajectories:  47%|‚ñç| 483/1024 [00:00<00:00, 946.16it/s, avg_reward=-28.1, episodes=1, st
Collecting Trajectories:  47%|‚ñç| 483/1024 [00:00<00:00, 946.16it/s, avg_reward=-74.4, episodes=0, st
Collecting Trajectories:  62%|‚ñå| 632/1024 [00:00<00:00, 1106.22it/s, avg_reward=-74.4, episodes=0, s
Collecting Trajectories:  62%|‚ñå| 632/1024 [00:00<00:00, 1106.22it/s, avg_reward=308.7, episodes=0, s
Collecting Trajectories:  73%|‚ñã| 751/1024 [00:00<00:00, 996.18it/s, avg_reward=308.7, episodes=0, st
Collecting Trajectories:  73%|‚ñã| 751/1024 [00:00<00:00, 996.18it/s, avg_reward=83.6, episodes=1, ste
Collecting Trajectories:  73%|‚ñã| 751/1024 [00:00<00:00, 996.18it/s, avg_reward=-30.8, episodes=0, st
Collecting Trajectories:  88%|‚ñâ| 901/1024 [00:00<00:00, 1133.39it/s, avg_reward=-30.8, episodes=0, s
Collecting Trajectories:  88%|‚ñâ| 901/1024 [00:01<00:00, 1133.39it/s, avg_reward=-116.1, episodes=1, 
                                                                                                    2025-06-25 01:44:41 [INFO    ] [Rank-22663] policy_gradient_methods: PPO training: epoch 4/4, batch 50/64, p_loss=0.024, v_loss=0.066, kl=0.0000

Collecting Trajectories:   0%|                                             | 0/1024 [00:00<?, ?it/s]
Collecting Trajectories:   0%| | 0/1024 [00:00<?, ?it/s, avg_reward=-137.5, episodes=0, step=1/1024]
Collecting Trajectories:   9%| | 92/1024 [00:00<00:01, 915.02it/s, avg_reward=-137.5, episodes=0, st
Collecting Trajectories:   9%| | 92/1024 [00:00<00:01, 915.02it/s, avg_reward=-9.3, episodes=0, step
Collecting Trajectories:   9%| | 92/1024 [00:00<00:01, 915.02it/s, avg_reward=-81.1, episodes=0, ste
Collecting Trajectories:  24%|‚ñè| 243/1024 [00:00<00:00, 1260.02it/s, avg_reward=-81.1, episodes=0, s
Collecting Trajectories:  24%|‚ñè| 243/1024 [00:00<00:00, 1260.02it/s, avg_reward=21.5, episodes=0, st
Collecting Trajectories:  39%|‚ñç| 398/1024 [00:00<00:00, 1391.66it/s, avg_reward=21.5, episodes=0, st
Collecting Trajectories:  39%|‚ñç| 398/1024 [00:00<00:00, 1391.66it/s, avg_reward=-36.0, episodes=1, s
Collecting Trajectories:  39%|‚ñç| 398/1024 [00:00<00:00, 1391.66it/s, avg_reward=25.5, episodes=0, st
Collecting Trajectories:  53%|‚ñå| 538/1024 [00:00<00:00, 1110.54it/s, avg_reward=25.5, episodes=0, st
Collecting Trajectories:  53%|‚ñå| 538/1024 [00:00<00:00, 1110.54it/s, avg_reward=-51.1, episodes=0, s
Collecting Trajectories:  67%|‚ñã| 687/1024 [00:00<00:00, 1225.57it/s, avg_reward=-51.1, episodes=0, s
Collecting Trajectories:  67%|‚ñã| 687/1024 [00:00<00:00, 1225.57it/s, avg_reward=-95.9, episodes=3, s
Collecting Trajectories:  67%|‚ñã| 687/1024 [00:00<00:00, 1225.57it/s, avg_reward=-116.9, episodes=0, 
Collecting Trajectories:  80%|‚ñä| 816/1024 [00:00<00:00, 1126.51it/s, avg_reward=-116.9, episodes=0, 
Collecting Trajectories:  80%|‚ñä| 816/1024 [00:00<00:00, 1126.51it/s, avg_reward=43.6, episodes=2, st
Collecting Trajectories:  93%|‚ñâ| 953/1024 [00:00<00:00, 1194.52it/s, avg_reward=43.6, episodes=2, st
Collecting Trajectories:  93%|‚ñâ| 953/1024 [00:00<00:00, 1194.52it/s, avg_reward=-32.4, episodes=0, s
                                                                                                    2025-06-25 01:44:42 [INFO    ] [Rank-22663] policy_gradient_methods: PPO training: epoch 4/4, batch 50/64, p_loss=0.027, v_loss=0.044, kl=0.0000

Collecting Trajectories:   0%|                                             | 0/1024 [00:00<?, ?it/s]
Collecting Trajectories:   0%| | 0/1024 [00:00<?, ?it/s, avg_reward=-112.5, episodes=0, step=1/1024]
Collecting Trajectories:   0%| | 0/1024 [00:00<?, ?it/s, avg_reward=-34.3, episodes=1, step=101/1024
Collecting Trajectories:  15%|‚ñè| 151/1024 [00:00<00:00, 1509.52it/s, avg_reward=-34.3, episodes=1, s
Collecting Trajectories:  15%|‚ñè| 151/1024 [00:00<00:00, 1509.52it/s, avg_reward=21.7, episodes=0, st
Collecting Trajectories:  15%|‚ñè| 151/1024 [00:00<00:00, 1509.52it/s, avg_reward=-9.8, episodes=0, st
Collecting Trajectories:  29%|‚ñé| 302/1024 [00:00<00:00, 1095.76it/s, avg_reward=-9.8, episodes=0, st
Collecting Trajectories:  29%|‚ñé| 302/1024 [00:00<00:00, 1095.76it/s, avg_reward=-11.3, episodes=1, s
Collecting Trajectories:  44%|‚ñç| 447/1024 [00:00<00:00, 1227.97it/s, avg_reward=-11.3, episodes=1, s
Collecting Trajectories:  44%|‚ñç| 447/1024 [00:00<00:00, 1227.97it/s, avg_reward=-86.3, episodes=1, s
Collecting Trajectories:  59%|‚ñå| 600/1024 [00:00<00:00, 1332.94it/s, avg_reward=-86.3, episodes=1, s
Collecting Trajectories:  59%|‚ñå| 600/1024 [00:00<00:00, 1332.94it/s, avg_reward=94.5, episodes=1, st
Collecting Trajectories:  59%|‚ñå| 600/1024 [00:00<00:00, 1332.94it/s, avg_reward=-0.1, episodes=1, st
Collecting Trajectories:  72%|‚ñã| 739/1024 [00:00<00:00, 1212.23it/s, avg_reward=-0.1, episodes=1, st
Collecting Trajectories:  72%|‚ñã| 739/1024 [00:00<00:00, 1212.23it/s, avg_reward=-54.0, episodes=1, s
Collecting Trajectories:  84%|‚ñä| 865/1024 [00:00<00:00, 1128.28it/s, avg_reward=-54.0, episodes=1, s
Collecting Trajectories:  84%|‚ñä| 865/1024 [00:00<00:00, 1128.28it/s, avg_reward=-78.4, episodes=0, s
Collecting Trajectories:  84%|‚ñä| 865/1024 [00:00<00:00, 1128.28it/s, avg_reward=149.7, episodes=0, s
Collecting Trajectories:  99%|‚ñâ| 1009/1024 [00:00<00:00, 1214.65it/s, avg_reward=149.7, episodes=0, 
                                                                                                    2025-06-25 01:44:43 [INFO    ] [Rank-22663] policy_gradient_methods: PPO training: epoch 4/4, batch 50/64, p_loss=0.033, v_loss=0.049, kl=0.0000

Collecting Trajectories:   0%|                                             | 0/1024 [00:00<?, ?it/s]
Collecting Trajectories:   0%|   | 0/1024 [00:00<?, ?it/s, avg_reward=-0.0, episodes=0, step=1/1024]
Collecting Trajectories:   0%| | 0/1024 [00:00<?, ?it/s, avg_reward=-90.5, episodes=0, step=101/1024
Collecting Trajectories:  15%|‚ñè| 152/1024 [00:00<00:00, 1513.29it/s, avg_reward=-90.5, episodes=0, s
Collecting Trajectories:  15%|‚ñè| 152/1024 [00:00<00:00, 1513.29it/s, avg_reward=24.9, episodes=1, st
Collecting Trajectories:  15%|‚ñè| 152/1024 [00:00<00:00, 1513.29it/s, avg_reward=60.5, episodes=0, st
Collecting Trajectories:  30%|‚ñé| 304/1024 [00:00<00:00, 1096.87it/s, avg_reward=60.5, episodes=0, st
Collecting Trajectories:  30%|‚ñé| 304/1024 [00:00<00:00, 1096.87it/s, avg_reward=-7.1, episodes=0, st
Collecting Trajectories:  45%|‚ñç| 456/1024 [00:00<00:00, 1254.43it/s, avg_reward=-7.1, episodes=0, st
Collecting Trajectories:  45%|‚ñç| 456/1024 [00:00<00:00, 1254.43it/s, avg_reward=-17.0, episodes=1, s
Collecting Trajectories:  45%|‚ñç| 456/1024 [00:00<00:00, 1254.43it/s, avg_reward=-109.1, episodes=0, 
Collecting Trajectories:  59%|‚ñå| 609/1024 [00:00<00:00, 1348.46it/s, avg_reward=-109.1, episodes=0, 
Collecting Trajectories:  59%|‚ñå| 609/1024 [00:00<00:00, 1348.46it/s, avg_reward=-11.6, episodes=0, s
Collecting Trajectories:  73%|‚ñã| 750/1024 [00:00<00:00, 1182.81it/s, avg_reward=-11.6, episodes=0, s
Collecting Trajectories:  73%|‚ñã| 750/1024 [00:00<00:00, 1182.81it/s, avg_reward=-31.4, episodes=1, s
Collecting Trajectories:  85%|‚ñä| 875/1024 [00:00<00:00, 1133.90it/s, avg_reward=-31.4, episodes=1, s
Collecting Trajectories:  85%|‚ñä| 875/1024 [00:00<00:00, 1133.90it/s, avg_reward=-22.0, episodes=0, s
Collecting Trajectories:  85%|‚ñä| 875/1024 [00:00<00:00, 1133.90it/s, avg_reward=22.5, episodes=0, st
Collecting Trajectories: 100%|‚ñâ| 1020/1024 [00:00<00:00, 1221.49it/s, avg_reward=22.5, episodes=0, s
                                                                                                    2025-06-25 01:44:44 [INFO    ] [Rank-22663] policy_gradient_methods: PPO training: epoch 4/4, batch 50/64, p_loss=-0.024, v_loss=0.052, kl=0.0000

Collecting Trajectories:   0%|                                             | 0/1024 [00:00<?, ?it/s]
Collecting Trajectories:   0%|    | 0/1024 [00:00<?, ?it/s, avg_reward=0.0, episodes=0, step=1/1024]
Collecting Trajectories:   0%| | 0/1024 [00:00<?, ?it/s, avg_reward=127.1, episodes=0, step=101/1024
Collecting Trajectories:  15%|‚ñè| 152/1024 [00:00<00:00, 1516.31it/s, avg_reward=127.1, episodes=0, s
Collecting Trajectories:  15%|‚ñè| 152/1024 [00:00<00:00, 1516.31it/s, avg_reward=-50.1, episodes=0, s
Collecting Trajectories:  15%|‚ñè| 152/1024 [00:00<00:00, 1516.31it/s, avg_reward=-120.6, episodes=1, 
Collecting Trajectories:  30%|‚ñé| 304/1024 [00:00<00:00, 1100.10it/s, avg_reward=-120.6, episodes=1, 
Collecting Trajectories:  30%|‚ñé| 304/1024 [00:00<00:00, 1100.10it/s, avg_reward=4.5, episodes=0, ste
Collecting Trajectories:  45%|‚ñç| 458/1024 [00:00<00:00, 1266.16it/s, avg_reward=4.5, episodes=0, ste
Collecting Trajectories:  45%|‚ñç| 458/1024 [00:00<00:00, 1266.16it/s, avg_reward=-111.0, episodes=1, 
Collecting Trajectories:  45%|‚ñç| 458/1024 [00:00<00:00, 1266.16it/s, avg_reward=-7.8, episodes=1, st
Collecting Trajectories:  60%|‚ñå| 615/1024 [00:00<00:00, 1371.47it/s, avg_reward=-7.8, episodes=1, st
Collecting Trajectories:  60%|‚ñå| 615/1024 [00:00<00:00, 1371.47it/s, avg_reward=-115.4, episodes=1, 
Collecting Trajectories:  74%|‚ñã| 758/1024 [00:00<00:00, 1193.06it/s, avg_reward=-115.4, episodes=1, 
Collecting Trajectories:  74%|‚ñã| 758/1024 [00:00<00:00, 1193.06it/s, avg_reward=-29.4, episodes=0, s
Collecting Trajectories:  86%|‚ñä| 884/1024 [00:00<00:00, 1159.56it/s, avg_reward=-29.4, episodes=0, s
Collecting Trajectories:  86%|‚ñä| 884/1024 [00:00<00:00, 1159.56it/s, avg_reward=-137.9, episodes=0, 
Collecting Trajectories:  86%|‚ñä| 884/1024 [00:00<00:00, 1159.56it/s, avg_reward=59.0, episodes=0, st
                                                                                                    2025-06-25 01:44:45 [INFO    ] [Rank-22663] policy_gradient_methods: PPO training: epoch 4/4, batch 50/64, p_loss=0.017, v_loss=0.061, kl=0.0000

Collecting Trajectories:   0%|                                             | 0/1024 [00:00<?, ?it/s]
Collecting Trajectories:   0%| | 0/1024 [00:00<?, ?it/s, avg_reward=-262.5, episodes=0, step=1/1024]
Collecting Trajectories:   0%| | 0/1024 [00:00<?, ?it/s, avg_reward=-66.6, episodes=0, step=101/1024
Collecting Trajectories:  15%|‚ñè| 151/1024 [00:00<00:00, 1509.96it/s, avg_reward=-66.6, episodes=0, s
Collecting Trajectories:  15%|‚ñè| 151/1024 [00:00<00:00, 1509.96it/s, avg_reward=-37.5, episodes=1, s
Collecting Trajectories:  15%|‚ñè| 151/1024 [00:00<00:00, 1509.96it/s, avg_reward=114.7, episodes=1, s
Collecting Trajectories:  29%|‚ñé| 302/1024 [00:00<00:00, 1090.71it/s, avg_reward=114.7, episodes=1, s
Collecting Trajectories:  29%|‚ñé| 302/1024 [00:00<00:00, 1090.71it/s, avg_reward=57.6, episodes=0, st
Collecting Trajectories:  44%|‚ñç| 454/1024 [00:00<00:00, 1251.29it/s, avg_reward=57.6, episodes=0, st
Collecting Trajectories:  44%|‚ñç| 454/1024 [00:00<00:00, 1251.29it/s, avg_reward=68.6, episodes=0, st
Collecting Trajectories:  44%|‚ñç| 454/1024 [00:00<00:00, 1251.29it/s, avg_reward=37.2, episodes=1, st
Collecting Trajectories:  59%|‚ñå| 606/1024 [00:00<00:00, 1344.58it/s, avg_reward=37.2, episodes=1, st
Collecting Trajectories:  59%|‚ñå| 606/1024 [00:00<00:00, 1344.58it/s, avg_reward=84.0, episodes=1, st
Collecting Trajectories:  73%|‚ñã| 746/1024 [00:00<00:00, 1178.68it/s, avg_reward=84.0, episodes=1, st
Collecting Trajectories:  73%|‚ñã| 746/1024 [00:00<00:00, 1178.68it/s, avg_reward=-30.4, episodes=1, s
Collecting Trajectories:  85%|‚ñä| 870/1024 [00:00<00:00, 1133.51it/s, avg_reward=-30.4, episodes=1, s
Collecting Trajectories:  85%|‚ñä| 870/1024 [00:00<00:00, 1133.51it/s, avg_reward=-71.5, episodes=1, s
Collecting Trajectories:  85%|‚ñä| 870/1024 [00:00<00:00, 1133.51it/s, avg_reward=54.0, episodes=0, st
Collecting Trajectories: 100%|‚ñâ| 1021/1024 [00:00<00:00, 1239.69it/s, avg_reward=54.0, episodes=0, s
                                                                                                    2025-06-25 01:44:46 [INFO    ] [Rank-22663] policy_gradient_methods: PPO training: epoch 4/4, batch 50/64, p_loss=-0.017, v_loss=0.060, kl=-0.0000

Collecting Trajectories:   0%|                                             | 0/1024 [00:00<?, ?it/s]
Collecting Trajectories:   0%| | 0/1024 [00:00<?, ?it/s, avg_reward=-225.0, episodes=0, step=1/1024]
Collecting Trajectories:   0%| | 0/1024 [00:00<?, ?it/s, avg_reward=-22.8, episodes=0, step=101/1024
Collecting Trajectories:  15%|‚ñè| 152/1024 [00:00<00:00, 1513.83it/s, avg_reward=-22.8, episodes=0, s
Collecting Trajectories:  15%|‚ñè| 152/1024 [00:00<00:00, 1513.83it/s, avg_reward=9.4, episodes=0, ste
Collecting Trajectories:  15%|‚ñè| 152/1024 [00:00<00:00, 1513.83it/s, avg_reward=75.7, episodes=0, st
Collecting Trajectories:  30%|‚ñé| 304/1024 [00:00<00:00, 1093.35it/s, avg_reward=75.7, episodes=0, st
Collecting Trajectories:  30%|‚ñé| 304/1024 [00:00<00:00, 1093.35it/s, avg_reward=207.1, episodes=2, s
Collecting Trajectories:  45%|‚ñç| 459/1024 [00:00<00:00, 1262.41it/s, avg_reward=207.1, episodes=2, s
Collecting Trajectories:  45%|‚ñç| 459/1024 [00:00<00:00, 1262.41it/s, avg_reward=47.4, episodes=0, st
Collecting Trajectories:  45%|‚ñç| 459/1024 [00:00<00:00, 1262.41it/s, avg_reward=-133.0, episodes=0, 
Collecting Trajectories:  60%|‚ñå| 614/1024 [00:00<00:00, 1361.41it/s, avg_reward=-133.0, episodes=0, 
Collecting Trajectories:  60%|‚ñå| 614/1024 [00:00<00:00, 1361.41it/s, avg_reward=176.9, episodes=1, s
Collecting Trajectories:  74%|‚ñã| 756/1024 [00:00<00:00, 1192.45it/s, avg_reward=176.9, episodes=1, s
Collecting Trajectories:  74%|‚ñã| 756/1024 [00:00<00:00, 1192.45it/s, avg_reward=-79.6, episodes=0, s
Collecting Trajectories:  86%|‚ñä| 882/1024 [00:00<00:00, 1125.09it/s, avg_reward=-79.6, episodes=0, s
Collecting Trajectories:  86%|‚ñä| 882/1024 [00:00<00:00, 1125.09it/s, avg_reward=-149.1, episodes=1, 
Collecting Trajectories:  86%|‚ñä| 882/1024 [00:00<00:00, 1125.09it/s, avg_reward=100.7, episodes=0, s
                                                                                                    2025-06-25 01:44:47 [INFO    ] [Rank-22663] policy_gradient_methods: PPO training: epoch 4/4, batch 50/64, p_loss=0.030, v_loss=0.060, kl=-0.0001

Collecting Trajectories:   0%|                                             | 0/1024 [00:00<?, ?it/s]
Collecting Trajectories:   0%| | 0/1024 [00:00<?, ?it/s, avg_reward=-587.5, episodes=0, step=1/1024]
Collecting Trajectories:   0%| | 0/1024 [00:00<?, ?it/s, avg_reward=-26.1, episodes=0, step=101/1024
Collecting Trajectories:  15%|‚ñè| 150/1024 [00:00<00:00, 1498.37it/s, avg_reward=-26.1, episodes=0, s
Collecting Trajectories:  15%|‚ñè| 150/1024 [00:00<00:00, 1498.37it/s, avg_reward=-15.1, episodes=0, s
Collecting Trajectories:  29%|‚ñé| 300/1024 [00:00<00:00, 1099.77it/s, avg_reward=-15.1, episodes=0, s
Collecting Trajectories:  29%|‚ñé| 300/1024 [00:00<00:00, 1099.77it/s, avg_reward=30.7, episodes=0, st
Collecting Trajectories:  29%|‚ñé| 300/1024 [00:00<00:00, 1099.77it/s, avg_reward=-17.1, episodes=1, s
Collecting Trajectories:  43%|‚ñç| 444/1024 [00:00<00:00, 1226.42it/s, avg_reward=-17.1, episodes=1, s
Collecting Trajectories:  43%|‚ñç| 444/1024 [00:00<00:00, 1226.42it/s, avg_reward=-32.0, episodes=1, s
Collecting Trajectories:  58%|‚ñå| 597/1024 [00:00<00:00, 1332.27it/s, avg_reward=-32.0, episodes=1, s
Collecting Trajectories:  58%|‚ñå| 597/1024 [00:00<00:00, 1332.27it/s, avg_reward=31.7, episodes=0, st
Collecting Trajectories:  58%|‚ñå| 597/1024 [00:00<00:00, 1332.27it/s, avg_reward=96.9, episodes=1, st
Collecting Trajectories:  72%|‚ñã| 736/1024 [00:00<00:00, 1192.23it/s, avg_reward=96.9, episodes=1, st
Collecting Trajectories:  72%|‚ñã| 736/1024 [00:00<00:00, 1192.23it/s, avg_reward=-116.3, episodes=0, 
Collecting Trajectories:  84%|‚ñä| 861/1024 [00:00<00:00, 1129.75it/s, avg_reward=-116.3, episodes=0, 
Collecting Trajectories:  84%|‚ñä| 861/1024 [00:00<00:00, 1129.75it/s, avg_reward=-10.1, episodes=0, s
Collecting Trajectories:  84%|‚ñä| 861/1024 [00:00<00:00, 1129.75it/s, avg_reward=39.9, episodes=0, st
Collecting Trajectories:  99%|‚ñâ| 1012/1024 [00:00<00:00, 1236.82it/s, avg_reward=39.9, episodes=0, s
                                                                                                    2025-06-25 01:44:48 [INFO    ] [Rank-22663] policy_gradient_methods: PPO training: epoch 4/4, batch 50/64, p_loss=-0.055, v_loss=0.044, kl=-0.0000

Collecting Trajectories:   0%|                                             | 0/1024 [00:00<?, ?it/s]
Collecting Trajectories:   0%|  | 0/1024 [00:00<?, ?it/s, avg_reward=-75.0, episodes=0, step=1/1024]
Collecting Trajectories:   0%| | 0/1024 [00:00<?, ?it/s, avg_reward=17.7, episodes=0, step=101/1024]
Collecting Trajectories:  15%|‚ñè| 153/1024 [00:00<00:00, 1524.54it/s, avg_reward=17.7, episodes=0, st
Collecting Trajectories:  15%|‚ñè| 153/1024 [00:00<00:00, 1524.54it/s, avg_reward=-17.1, episodes=0, s
Collecting Trajectories:  15%|‚ñè| 153/1024 [00:00<00:00, 1524.54it/s, avg_reward=-43.9, episodes=1, s
Collecting Trajectories:  30%|‚ñé| 306/1024 [00:00<00:00, 1109.18it/s, avg_reward=-43.9, episodes=1, s
Collecting Trajectories:  30%|‚ñé| 306/1024 [00:00<00:00, 1109.18it/s, avg_reward=33.4, episodes=0, st
Collecting Trajectories:  45%|‚ñç| 458/1024 [00:00<00:00, 1264.68it/s, avg_reward=33.4, episodes=0, st
Collecting Trajectories:  45%|‚ñç| 458/1024 [00:00<00:00, 1264.68it/s, avg_reward=55.0, episodes=0, st
Collecting Trajectories:  45%|‚ñç| 458/1024 [00:00<00:00, 1264.68it/s, avg_reward=33.4, episodes=1, st
Collecting Trajectories:  59%|‚ñå| 606/1024 [00:00<00:00, 1338.59it/s, avg_reward=33.4, episodes=1, st