/home/alex/miniconda3/envs/GeneticTrading/bin/python /mnt/windows/GeneticTrading/run_simple.py 
ðŸ¤– Trading System Launcher
==================================================
âœ“ Created directory: ./models
âœ“ Created directory: ./logs
âœ“ Created directory: ./cached_data
âœ“ Created directory: ./runs

Available modes:
  test - Quick test with minimal data (default)
  dev  - Development mode with 10% data

Usage: python run_simple.py [test|dev]

Select mode (test/dev) [test]: dev
ðŸ”§ Starting Development Mode...
ðŸ“Š Using 10% of data for development
WARNING:root:Distributed training not available, running in single-process mode
2025-06-25 00:39:22 [INFO    ] [Rank-13197] STARTUP             : ================================================================================
2025-06-25 00:39:22 [INFO    ] [Rank-13197] STARTUP             : Training session started for rank 0
2025-06-25 00:39:22 [INFO    ] [Rank-13197] STARTUP             : Log file: logs/training_rank_0.log
2025-06-25 00:39:22 [INFO    ] [Rank-13197] STARTUP             : Process ID: 13197
2025-06-25 00:39:22 [INFO    ] [Rank-13197] STARTUP             : Working directory: /mnt/windows/GeneticTrading
2025-06-25 00:39:22 [INFO    ] [Rank-13197] STARTUP             : ================================================================================
2025-06-25 00:39:22 [INFO    ] [Rank-13197] root                : NCCL_TIMEOUT = 1800000 ms
2025-06-25 00:39:22 [INFO    ] [Rank-13197] root                : Using 10.0% of available data
2025-06-25 00:39:22 [INFO    ] [Rank-13197] root                : Models will be saved to: ./models/dev
2025-06-25 00:39:22 [INFO    ] [Rank-13197] root                : Rank 0/1 starting on cuda:0 (has_cudf=True)
2025-06-25 00:39:22 [INFO    ] [Rank-13197] root                : Parquet cache found; skipping preprocessing.
2025-06-25 00:39:22 [INFO    ] [Rank-13197] root                : Total data: 4311800 train, 1077950 test rows
2025-06-25 00:39:22 [INFO    ] [Rank-13197] root                : Rank 0: Sampled 100000 train rows from 4311800 total
2025-06-25 00:39:22 [INFO    ] [Rank-13197] root                : Rank 0: Sampled 20000 test rows from 1077950 total
2025-06-25 00:39:22 [INFO    ] [Rank-13197] numba.cuda.cudadrv.driver: init
2025-06-25 00:39:24 [INFO    ] [Rank-13197] adaptive_trainer    : Starting adaptive training
2025-06-25 00:39:24 [INFO    ] [Rank-13197] adaptive_trainer    : 
=== Adaptive Training Iteration 1/20 ===
2025-06-25 00:39:24 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:39:28 [INFO    ] [Rank-13197] adaptive_trainer    : Evaluation results: 18439 profits, total=759434508.9795
2025-06-25 00:39:28 [INFO    ] [Rank-13197] adaptive_trainer    : Metrics: CAGR=138.6187, Sharpe=5.0000, MDD=0.0000
2025-06-25 00:39:28 [INFO    ] [Rank-13197] adaptive_trainer    : Current performance: 0.6659 (best: 0.6659)
2025-06-25 00:39:28 [INFO    ] [Rank-13197] adaptive_trainer    : Stagnation: 0, Poor performance: 0
2025-06-25 00:39:28 [INFO    ] [Rank-13197] adaptive_trainer    : Method: GA, Entropy: 1.0986
2025-06-25 00:39:28 [INFO    ] [Rank-13197] adaptive_trainer    : Switching to PPO due to: good_performance_refinement, ga_solution_refinement
2025-06-25 00:39:28 [INFO    ] [Rank-13197] adaptive_trainer    : Switching from GA to PPO
2025-06-25 00:39:28 [INFO    ] [Rank-13197] adaptive_trainer    : Starting PPO phase: 150 updates
2025-06-25 00:39:28 [INFO    ] [Rank-13197] policy_gradient_methods: Loaded model from models/dev/ppo_models/adaptive_ppo_model.pth
I0625 00:39:28.921000 13197 site-packages/torch/distributed/nn/jit/instantiator.py:24] Created a temporary directory at /tmp/tmp_yjsmzc8
I0625 00:39:28.922000 13197 site-packages/torch/distributed/nn/jit/instantiator.py:75] Writing /tmp/tmp_yjsmzc8/_remote_module_non_scriptable.py
Removed old TensorBoard run: ./runs/ppo_rank_0
2025-06-25 00:39:29 [INFO    ] [Rank-13197] policy_gradient_methods: Loaded model from models/dev/ppo_models/adaptive_ppo_model.pth
2025-06-25 00:39:29 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:39:29 [INFO    ] [Rank-13197] adaptive_trainer    : Evaluation results: 1617 profits, total=-0.3187
2025-06-25 00:39:29 [INFO    ] [Rank-13197] adaptive_trainer    : Metrics: CAGR=-0.0021, Sharpe=-5.0000, MDD=100.0000
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:39:29 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:39:30 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:39:30 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:39:30 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:39:30 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:39:31 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite observations detected, skipping update
PPO Update Epochs:   0%|          | 0/4 [00:00<?, ?it/s]
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s]
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0341, value_loss=26.9302, entropy=1.0928]
Epoch 1 Batches:   6%|â–‹         | 1/16 [00:00<00:01,  8.43it/s, policy_loss=-0.0341, value_loss=26.9302, entropy=1.0928]
Epoch 1 Batches:   6%|â–‹         | 1/16 [00:00<00:01,  8.43it/s, policy_loss=-0.2604, value_loss=26.6795, entropy=1.0938]
Epoch 1 Batches:   6%|â–‹         | 1/16 [00:00<00:01,  8.43it/s, policy_loss=-0.0166, value_loss=26.1309, entropy=1.0932]
Epoch 1 Batches:   6%|â–‹         | 1/16 [00:00<00:01,  8.43it/s, policy_loss=-0.0348, value_loss=19.2409, entropy=1.0945]
Epoch 1 Batches:   6%|â–‹         | 1/16 [00:00<00:01,  8.43it/s, policy_loss=0.0293, value_loss=17.2157, entropy=1.0951] 
Epoch 1 Batches:   6%|â–‹         | 1/16 [00:00<00:01,  8.43it/s, policy_loss=0.1032, value_loss=19.6318, entropy=1.0948]
Epoch 1 Batches:   6%|â–‹         | 1/16 [00:00<00:01,  8.43it/s, policy_loss=0.1012, value_loss=25.3895, entropy=1.0949]
Epoch 1 Batches:   6%|â–‹         | 1/16 [00:00<00:01,  8.43it/s, policy_loss=0.0637, value_loss=24.9728, entropy=1.0940]
Epoch 1 Batches:   6%|â–‹         | 1/16 [00:00<00:01,  8.43it/s, policy_loss=0.0915, value_loss=23.4544, entropy=1.0939]
Epoch 1 Batches:   6%|â–‹         | 1/16 [00:00<00:01,  8.43it/s, policy_loss=-0.0448, value_loss=29.6418, entropy=1.0933]
Epoch 1 Batches:   6%|â–‹         | 1/16 [00:00<00:01,  8.43it/s, policy_loss=0.0698, value_loss=23.8028, entropy=1.0929] 
Epoch 1 Batches:   6%|â–‹         | 1/16 [00:00<00:01,  8.43it/s, policy_loss=-0.0421, value_loss=26.1809, entropy=1.0933]
Epoch 1 Batches:   6%|â–‹         | 1/16 [00:00<00:01,  8.43it/s, policy_loss=0.1915, value_loss=24.1524, entropy=1.0926] 
Epoch 1 Batches:   6%|â–‹         | 1/16 [00:00<00:01,  8.43it/s, policy_loss=-0.1678, value_loss=27.0353, entropy=1.0931]
Epoch 1 Batches:   6%|â–‹         | 1/16 [00:00<00:01,  8.43it/s, policy_loss=-0.0373, value_loss=25.6454, entropy=1.0929]
Epoch 1 Batches:   6%|â–‹         | 1/16 [00:00<00:01,  8.43it/s, policy_loss=0.0300, value_loss=29.9200, entropy=1.0935] 
PPO Update Epochs:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  6.10it/s, avg_policy_loss=0.0026, avg_value_loss=24.7515, kl_div=0.0004]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0079, value_loss=24.3436, entropy=1.0940]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0811, value_loss=20.2579, entropy=1.0939]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1360, value_loss=25.1500, entropy=1.0934]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1996, value_loss=27.4262, entropy=1.0938]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1614, value_loss=35.2756, entropy=1.0940]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0530, value_loss=22.9758, entropy=1.0943] 
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0424, value_loss=18.7984, entropy=1.0941]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0401, value_loss=25.3967, entropy=1.0948]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0789, value_loss=30.8147, entropy=1.0947] 
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0040, value_loss=15.3089, entropy=1.0940]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0213, value_loss=21.1174, entropy=1.0938]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1324, value_loss=28.5704, entropy=1.0942]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0476, value_loss=24.0525, entropy=1.0939]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.3333, value_loss=29.1936, entropy=1.0942] 
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0077, value_loss=25.3532, entropy=1.0937]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1081, value_loss=21.4368, entropy=1.0931] 
PPO Update Epochs:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  6.10it/s, avg_policy_loss=0.0014, avg_value_loss=24.7170, kl_div=-0.0005]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.3091, value_loss=28.5168, entropy=1.0942]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0617, value_loss=23.6889, entropy=1.0937]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1586, value_loss=20.8137, entropy=1.0929] 
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1111, value_loss=26.5854, entropy=1.0938]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1270, value_loss=30.0569, entropy=1.0944]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0480, value_loss=23.3598, entropy=1.0939]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1039, value_loss=23.0526, entropy=1.0940] 
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0067, value_loss=20.7306, entropy=1.0942]
Epoch 3 Batches:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:00<00:00, 67.18it/s, policy_loss=-0.0067, value_loss=20.7306, entropy=1.0942]
Epoch 3 Batches:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:00<00:00, 67.18it/s, policy_loss=-0.1065, value_loss=27.9326, entropy=1.0943]
Epoch 3 Batches:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:00<00:00, 67.18it/s, policy_loss=-0.1208, value_loss=22.7017, entropy=1.0938]
Epoch 3 Batches:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:00<00:00, 67.18it/s, policy_loss=0.0525, value_loss=29.0784, entropy=1.0936] 
Epoch 3 Batches:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:00<00:00, 67.18it/s, policy_loss=-0.0849, value_loss=22.0801, entropy=1.0936]
Epoch 3 Batches:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:00<00:00, 67.18it/s, policy_loss=0.1647, value_loss=23.2301, entropy=1.0940] 
Epoch 3 Batches:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:00<00:00, 67.18it/s, policy_loss=-0.1002, value_loss=23.6391, entropy=1.0935]
Epoch 3 Batches:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:00<00:00, 67.18it/s, policy_loss=0.1500, value_loss=26.6571, entropy=1.0926] 
Epoch 3 Batches:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:00<00:00, 67.18it/s, policy_loss=0.2404, value_loss=23.3312, entropy=1.0940]
PPO Update Epochs:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  9.41it/s, avg_policy_loss=0.0010, avg_value_loss=24.7159, kl_div=-0.0009]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0654, value_loss=26.9114, entropy=1.0937]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.2566, value_loss=17.9401, entropy=1.0921] 
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0959, value_loss=28.6714, entropy=1.0936]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0329, value_loss=26.9978, entropy=1.0936]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0326, value_loss=23.5822, entropy=1.0941] 
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1452, value_loss=26.1458, entropy=1.0938]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0228, value_loss=23.3186, entropy=1.0941]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0957, value_loss=21.0938, entropy=1.0935]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.2477, value_loss=32.5007, entropy=1.0932]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0418, value_loss=29.6348, entropy=1.0935]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1717, value_loss=21.7803, entropy=1.0941] 
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1293, value_loss=23.4876, entropy=1.0928]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0445, value_loss=24.3345, entropy=1.0924]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1723, value_loss=24.8113, entropy=1.0924] 
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0454, value_loss=18.6837, entropy=1.0919]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0473, value_loss=25.3358, entropy=1.0917] 
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:39:32 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:39:33 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite observations detected, skipping update
PPO Update Epochs:   0%|          | 0/4 [00:00<?, ?it/s]
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s]
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0612, value_loss=28.6736, entropy=1.0920]
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0251, value_loss=19.6245, entropy=1.0922]
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0392, value_loss=28.2926, entropy=1.0903]
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0023, value_loss=23.7878, entropy=1.0920]
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1534, value_loss=28.2663, entropy=1.0922]
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1072, value_loss=27.7707, entropy=1.0924] 
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0928, value_loss=28.6736, entropy=1.0910]
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1295, value_loss=25.3856, entropy=1.0924] 
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0947, value_loss=24.1024, entropy=1.0929]
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1098, value_loss=29.1861, entropy=1.0923]
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1770, value_loss=29.1243, entropy=1.0917] 
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1448, value_loss=30.3298, entropy=1.0918]
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0214, value_loss=28.3741, entropy=1.0918]
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0848, value_loss=31.7868, entropy=1.0914]
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0851, value_loss=29.5068, entropy=1.0914] 
Epoch 1 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0388, value_loss=32.9607, entropy=1.0918]
PPO Update Epochs:   0%|          | 0/4 [00:00<?, ?it/s, avg_policy_loss=0.0025, avg_value_loss=27.8654, kl_div=-0.0012]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0144, value_loss=26.1563, entropy=1.0906]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0809, value_loss=27.1384, entropy=1.0913]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0050, value_loss=32.1653, entropy=1.0908]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.2092, value_loss=28.2020, entropy=1.0906]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1552, value_loss=31.7542, entropy=1.0909] 
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1051, value_loss=24.8952, entropy=1.0913]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1415, value_loss=28.2601, entropy=1.0898]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0597, value_loss=24.8990, entropy=1.0893]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0144, value_loss=29.4179, entropy=1.0905] 
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1233, value_loss=31.4496, entropy=1.0898]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0494, value_loss=26.4880, entropy=1.0905]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0214, value_loss=27.4188, entropy=1.0900]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0708, value_loss=22.0666, entropy=1.0888]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0661, value_loss=26.1004, entropy=1.0897] 
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0623, value_loss=25.1927, entropy=1.0894]
Epoch 2 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0701, value_loss=33.5093, entropy=1.0889] 
PPO Update Epochs:   0%|          | 0/4 [00:00<?, ?it/s, avg_policy_loss=0.0013, avg_value_loss=27.8196, kl_div=0.0001] 
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0358, value_loss=23.2379, entropy=1.0888]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1486, value_loss=26.3213, entropy=1.0893] 
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1091, value_loss=28.8587, entropy=1.0916]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0129, value_loss=25.1863, entropy=1.0905]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0201, value_loss=25.4481, entropy=1.0912]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1168, value_loss=24.5096, entropy=1.0904] 
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1288, value_loss=30.2570, entropy=1.0889]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0951, value_loss=30.4786, entropy=1.0902]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1547, value_loss=26.5613, entropy=1.0893] 
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1821, value_loss=37.0518, entropy=1.0900]
Epoch 3 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1968, value_loss=22.4500, entropy=1.0900]
Epoch 3 Batches:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:00<00:00, 106.67it/s, policy_loss=-0.1968, value_loss=22.4500, entropy=1.0900]
Epoch 3 Batches:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:00<00:00, 106.67it/s, policy_loss=-0.0823, value_loss=24.6798, entropy=1.0877]
Epoch 3 Batches:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:00<00:00, 106.67it/s, policy_loss=0.1150, value_loss=29.3489, entropy=1.0879] 
Epoch 3 Batches:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:00<00:00, 106.67it/s, policy_loss=0.0439, value_loss=31.6889, entropy=1.0854]
Epoch 3 Batches:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:00<00:00, 106.67it/s, policy_loss=-0.0697, value_loss=27.9512, entropy=1.0868]
Epoch 3 Batches:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:00<00:00, 106.67it/s, policy_loss=-0.1305, value_loss=30.9644, entropy=1.0870]
PPO Update Epochs:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00, 15.84it/s, avg_policy_loss=-0.0005, avg_value_loss=27.8121, kl_div=0.0007]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1407, value_loss=25.1727, entropy=1.0856]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1551, value_loss=25.6863, entropy=1.0865]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0154, value_loss=25.6000, entropy=1.0869] 
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1308, value_loss=28.1130, entropy=1.0834]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.0169, value_loss=28.7669, entropy=1.0869]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0931, value_loss=26.5156, entropy=1.0875]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.2324, value_loss=25.1661, entropy=1.0889] 
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1247, value_loss=31.0745, entropy=1.0850]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.2995, value_loss=29.0963, entropy=1.0865] 
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.1989, value_loss=26.5781, entropy=1.0868]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=0.3739, value_loss=28.1030, entropy=1.0882]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.2734, value_loss=26.8247, entropy=1.0892]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1337, value_loss=28.5926, entropy=1.0897]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.0989, value_loss=32.2589, entropy=1.0869]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1584, value_loss=28.0995, entropy=1.0881]
Epoch 4 Batches:   0%|          | 0/16 [00:00<?, ?it/s, policy_loss=-0.1756, value_loss=29.6281, entropy=1.0889]
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:39:33 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:39:34 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:39:34 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:39:34 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:39:34 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:39:35 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:39:35 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:39:35 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:39:35 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:39:36 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:39:36 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:39:36 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:39:36 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:39:37 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:39:37 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:39:37 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:39:37 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:39:38 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:39:38 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:39:38 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:39:38 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:39:39 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:39:39 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
2025-06-25 00:39:39 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite observations detected, skipping update
Collecting trajectories:   0%|          | 0/1024 [00:00<?, ?it/s]2025-06-25 00:39:39 [WARNING ] [Rank-13197] policy_gradient_methods: NaN/infinite input detected, replacing with zeros
